{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pré-processamento e Engenharia de Features\n",
    "\n",
    "Este notebook implementa as etapas de **Limpeza, Transformação e Preparação para Modelagem** conforme a Parte 2 da avaliação, incluindo a codificação de variáveis categóricas e a divisão dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Caminho para o dataset\n",
    "DATA_PATH = \"../data/Housing.csv\"\n",
    "\n",
    "# 1. Carregamento dos Dados\n",
    "print(\"--- 1. Carregamento dos Dados ---\")\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Dataset carregado. Formato: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Arquivo não encontrado em {DATA_PATH}.\")\n",
    "    exit()\n",
    "\n",
    "# 2. Codificação de Variáveis Categóricas\n",
    "print(\"\\n--- 2. Codificação de Variáveis Categóricas ---\")\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "# O dataset possui colunas binárias (yes/no) e colunas com mais de duas categorias (furnishingstatus, prefarea, hotwaterheating, airconditioning, etc.)\n",
    "# Vamos usar One-Hot Encoding (get_dummies) para todas as colunas categóricas para simplificar.\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "print(f\"Dataset após One-Hot Encoding. Formato: {df_encoded.shape}\")\n",
    "print(\"Novas colunas criadas:\")\n",
    "print(df_encoded.columns.difference(df.columns))\n",
    "\n",
    "# 3. Separação de Variáveis (X e y)\n",
    "print(\"\\n--- 3. Separação de Variáveis (X e y) ---\")\n",
    "X = df_encoded.drop('price', axis=1)\n",
    "y = df_encoded['price']\n",
    "print(f\"Variáveis preditoras (X) shape: {X.shape}\")\n",
    "print(f\"Variável alvo (y) shape: {y.shape}\")\n",
    "\n",
    "# 4. Normalização/Escalonamento de Variáveis Numéricas\n",
    "print(\"\\n--- 4. Normalização/Escalonamento de Variáveis Numéricas ---\")\n",
    "# A coluna 'price' (y) não será escalonada neste momento, apenas as features (X)\n",
    "numeric_cols = X.select_dtypes(include=np.number).columns\n",
    "scaler = MinMaxScaler()\n",
    "X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
    "print(\"Variáveis numéricas em X escalonadas com MinMaxScaler.\")\n",
    "\n",
    "# 5. Divisão em Conjuntos de Treino e Teste\n",
    "print(\"\\n--- 5. Divisão em Conjuntos de Treino e Teste ---\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# 6. Salvando os dados pré-processados para a próxima etapa\n",
    "print(\"\\n--- 6. Salvando os dados pré-processados ---\")\n",
    "np.savez_compressed('../data/processed_data.npz', \n",
    "                     X_train=X_train.values, X_test=X_test.values, \n",
    "                     y_train=y_train.values, y_test=y_test.values, \n",
    "                     feature_names=X_train.columns.values)\n",
    "print(\"Dados de treino e teste salvos em ../data/processed_data.npz\")\n",
    "\n",
    "# Salvando o código do notebook em um arquivo .py para execução no shell\n",
    "with open('../notebooks/02_Pre_processamento_e_Features.py', 'w') as f:\n",
    "    f.write(\"\"\"import pandas as pd\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import MinMaxScaler\\nimport os\\n\\nDATA_PATH = \"data/Housing.csv\"\\n\\ntry:\\n    df = pd.read_csv(DATA_PATH)\\nexcept FileNotFoundError:\\n    print(f\"Erro: Arquivo não encontrado em {DATA_PATH}.\")\\n    exit()\\n\\n# Codificação de Variáveis Categóricas\\ncategorical_cols = df.select_dtypes(include='object').columns\\ndf_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\\n\\n# Separação de Variáveis (X e y)\\nX = df_encoded.drop('price', axis=1)\\ny = df_encoded['price']\\n\\n# Normalização/Escalonamento de Variáveis Numéricas\\nnumeric_cols = X.select_dtypes(include=np.number).columns\\nscaler = MinMaxScaler()\\nX[numeric_cols] = scaler.fit_transform(X[numeric_cols])\\n\\n# Divisão em Conjuntos de Treino e Teste\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\\n\\n# Salvando os dados pré-processados para a próxima etapa\\nnp.savez_compressed('data/processed_data.npz', \\n                     X_train=X_train.values, X_test=X_test.values, \\n                     y_train=y_train.values, y_test=y_test.values, \\n                     feature_names=X_train.columns.values)\\n\\nprint(\"Pré-processamento concluído. Dados de treino e teste salvos em data/processed_data.npz\")\\n\"\"\")\n",
    "print(\"Script Python 02_Pre_processamento_e_Features.py salvo para execução.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
